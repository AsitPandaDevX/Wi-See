{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "MDhxgcTvig6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import timeit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "# from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "from pathlib import Path\n",
        "from math import sqrt\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "id": "NC1OL14NgiQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVqs7Gtwfe_m"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q3qHQ28sgMp"
      },
      "source": [
        "# Data File Count & Duration"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CSI_data gdrive URL\n",
        "data_url = '/content/drive/MyDrive/CSI_Training_Data'\n",
        "\n",
        "raw_np = []\n",
        "raw_np_duration = []\n",
        "raw_wp = []\n",
        "raw_wp_duration = []\n",
        "\n",
        "# Get the list of CSV files\n",
        "file_list = glob.glob(data_url + '/*.csv')\n",
        "\n",
        "print(\"Total File Count:\", len(file_list))\n",
        "print(\"==================================================================\")\n",
        "\n",
        "# Process files with 'NP' in their name\n",
        "print(\"Processing no-person (NP) files:\")\n",
        "for file_name in tqdm(file_list, desc=\"No-Person Files\", unit=\"file\"):\n",
        "    if 'NP' in file_name:\n",
        "        try:\n",
        "            raw_np.append(file_name)\n",
        "            extract_duration_np = pd.read_csv(file_name, low_memory=False)\n",
        "            extract_duration_np = extract_duration_np['real_timestamp'].values\n",
        "            extract_duration_np = (extract_duration_np[-1] - extract_duration_np[0]) / 60\n",
        "            raw_np_duration.append(extract_duration_np)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {file_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "print(\"==================================================================\")\n",
        "print(\"Found {} (no-person) data files.\".format(len(raw_np)))\n",
        "print(\"With a total duration of {} minutes.\".format(sum(raw_np_duration)))\n",
        "print(\"==================================================================\")\n",
        "\n",
        "# Process files without 'NP' in their name\n",
        "print(\"Processing with-person (WP) files:\")\n",
        "for file_name in tqdm(file_list, desc=\"With-Person Files\", unit=\"file\"):\n",
        "    if 'NP' not in file_name:\n",
        "        try:\n",
        "            raw_wp.append(file_name)\n",
        "            extract_duration_wp = pd.read_csv(file_name, low_memory=False)\n",
        "            extract_duration_wp = extract_duration_wp['real_timestamp'].values\n",
        "            extract_duration_wp = (extract_duration_wp[-1] - extract_duration_wp[0]) / 60\n",
        "            raw_wp_duration.append(extract_duration_wp)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {file_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "print(\"\\n==================================================================\")\n",
        "print(\"Found {} (with-person) data files.\".format(len(raw_wp)))\n",
        "print(\"With a total duration of {} minutes.\".format(sum(raw_wp_duration)))\n",
        "print(\"==================================================================\")\n"
      ],
      "metadata": {
        "id": "7kl8Ymjw_gml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LuJus_Ar8iY"
      },
      "source": [
        "# Filter Design\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUyMv3g7Fcr6"
      },
      "outputs": [],
      "source": [
        "def lowpass(csi_vec: np.array, cutoff: float, fs: float, order: int) -> np.array:\n",
        "    nyq = 0.5*fs\n",
        "    normal_cutoff = cutoff/nyq\n",
        "    b, a = signal.butter(order, normal_cutoff, btype=\"low\", analog=False)\n",
        "    return signal.filtfilt(b, a, csi_vec)\n",
        "\n",
        "def running_mean(x: np.array, N: int) -> np.array:\n",
        "    return pd.Series(x).rolling(window=N, min_periods=1, center=True).mean().to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y720Xk0_tM0l"
      },
      "source": [
        "#  Read File Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yueUbOIzEUD2"
      },
      "outputs": [],
      "source": [
        "def read_csv(data_url, window_size, step_size):\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    data_np_windowed = []\n",
        "    data_wp_windowed = []\n",
        "\n",
        "    data_np = []\n",
        "    data_wp = []\n",
        "\n",
        "    label_data_np = []\n",
        "    label_data_wp = []\n",
        "\n",
        "    file_list = glob.glob(data_url + '/*.csv')\n",
        "\n",
        "    # File processing progress\n",
        "    for file_name in tqdm(file_list, desc=\"Processing Files\", unit=\"file\"):\n",
        "        print(file_name)\n",
        "        if file_name.find('NP') != -1:\n",
        "            noPersonData = pd.read_csv(file_name, low_memory=False)\n",
        "            noPersonData = noPersonData['CSI_DATA'].values\n",
        "            noPersonData = noPersonData[500:-500]\n",
        "\n",
        "            # Data processing progress\n",
        "            for i in tqdm(range(noPersonData.shape[0]), desc=\"Processing No-Person Data\", leave=False):\n",
        "                try:\n",
        "                    st = noPersonData[i]\n",
        "                    st = st[1:-2]\n",
        "                    data_array = [int(s) for s in st.split(' ')]\n",
        "                    data_array_mag = []\n",
        "                    for k in range(0, 128, 2):\n",
        "                        data_array_mag.append(sqrt(data_array[k]**2 + data_array[k+1]**2))\n",
        "                    data_np.append(data_array_mag)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error encountered: {e}\")\n",
        "                    continue\n",
        "                except ValueError as ve:\n",
        "                    print(f\"ValueError encountered: {ve}\")\n",
        "                    continue\n",
        "        else:\n",
        "            withPersonData = pd.read_csv(file_name, low_memory=False)\n",
        "            withPersonData = withPersonData['CSI_DATA'].values\n",
        "            withPersonData = withPersonData[500:-500]\n",
        "\n",
        "            # Data processing progress\n",
        "            for i in tqdm(range(withPersonData.shape[0]), desc=\"Processing With-Person Data\", leave=False):\n",
        "                try:\n",
        "                    st = withPersonData[i]\n",
        "                    st = st[1:-2]\n",
        "                    data_array = [int(s) for s in st.split(' ')]\n",
        "                    data_array_mag = []\n",
        "                    for k in range(0, 128, 2):\n",
        "                        data_array_mag.append(sqrt(data_array[k]**2 + data_array[k+1]**2))\n",
        "                    data_wp.append(data_array_mag)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error encountered: {e}\")\n",
        "                    continue\n",
        "                except ValueError as ve:\n",
        "                    print(f\"ValueError encountered: {ve}\")\n",
        "                    continue\n",
        "\n",
        "    data_np = np.array(data_np)\n",
        "    data_wp = np.array(data_wp)\n",
        "\n",
        "    # Window processing progress for no-person data\n",
        "    for start in tqdm(range(0, data_np.shape[0] - window_size, step_size), desc=\"Creating No-Person Windows\", unit=\"window\"):\n",
        "        end = start + window_size\n",
        "        data_np_window = np.empty((0, 64))\n",
        "        for j in range(start, end):\n",
        "            data_array_mag = data_np[j]\n",
        "            data_np_window = np.append(data_np_window, np.array([data_array_mag]), axis=0)\n",
        "        for i in range(0, 64):\n",
        "            data_np_window[:, i] = lowpass(data_np_window[:, i], 30, 170, 5)\n",
        "            data_np_window[:, i] = running_mean(data_np_window[:, i], 10)\n",
        "        data_np_window = scaler.fit_transform(data_np_window)\n",
        "        data_np_windowed.append(data_np_window)\n",
        "        label_data_np.append(0)\n",
        "\n",
        "    # Window processing progress for with-person data\n",
        "    for start in tqdm(range(0, data_wp.shape[0] - window_size, step_size), desc=\"Creating With-Person Windows\", unit=\"window\"):\n",
        "        end = start + window_size\n",
        "        data_wp_window = np.empty((0, 64))\n",
        "        for j in range(start, end):\n",
        "            data_array_mag = data_wp[j]\n",
        "            data_wp_window = np.append(data_wp_window, np.array([data_array_mag]), axis=0)\n",
        "        for i in range(0, 64):\n",
        "            data_wp_window[:, i] = lowpass(data_wp_window[:, i], 30, 170, 5)\n",
        "            data_wp_window[:, i] = running_mean(data_wp_window[:, i], 10)\n",
        "        data_wp_window = scaler.fit_transform(data_wp_window)\n",
        "        data_wp_windowed.append(data_wp_window)\n",
        "        label_data_wp.append(1)\n",
        "\n",
        "    data_np = data_np_windowed\n",
        "    data_wp = data_wp_windowed\n",
        "\n",
        "    return np.array(data_np), np.array(data_wp), np.array(label_data_np), np.array(label_data_wp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Extraction"
      ],
      "metadata": {
        "id": "pfh1VKnEUAZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 200\n",
        "step_size = 50\n",
        "\n",
        "save_url_np = 'data_np_window_size={window}_step_size={step}.npy'.format(window = window_size, step = step_size)\n",
        "save_url_np = Path(save_url_np)\n",
        "save_url_wp = 'data_wp_window_size={window}_step_size={step}.npy'.format(window = window_size, step = step_size)\n",
        "save_url_wp = Path(save_url_wp)\n",
        "\n",
        "if (save_url_np.is_file() and save_url_wp.is_file()):\n",
        "  data_np = np.load(save_url_np)\n",
        "  data_wp = np.load(save_url_wp)\n",
        "  label_data_np = np.zeros((data_np.shape[0],1))\n",
        "  label_data_wp = np.zeros((data_wp.shape[0],1))\n",
        "  label_data_np[:,0] = 0\n",
        "  label_data_wp[:,0] = 1\n",
        "else:\n",
        "  file1 = 'data_np_window_size={window}_step_size={step}.npy'.format(window = window_size, step = step_size)\n",
        "  file2 = 'data_wp_window_size={window}_step_size={step}.npy'.format(window = window_size, step = step_size)\n",
        "  data_np, data_wp, label_data_np, label_data_wp = read_csv(data_url, window_size=window_size, step_size=step_size)\n",
        "  np.save(file1,data_np)\n",
        "  np.save(file2,data_wp)"
      ],
      "metadata": {
        "id": "ulmhhN49I3gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_np.shape)\n",
        "print(data_wp.shape)\n",
        "print(label_data_np.shape)\n",
        "print(label_data_wp.shape)"
      ],
      "metadata": {
        "id": "S2NS0uW1KkNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IwBA6McCWyv"
      },
      "source": [
        "# *Train Test Split*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0XsonEYEdsP"
      },
      "outputs": [],
      "source": [
        "def train_test_split(data_np, data_wp, label_data_np, label_data_wp,train_portion, test_portion):\n",
        "  x_train = []\n",
        "  x_valid = []\n",
        "  x_test = []\n",
        "\n",
        "  y_train = []\n",
        "  y_valid = []\n",
        "  y_test = []\n",
        "\n",
        "  index1 = np.random.permutation([i for i in range(data_np.shape[0])])\n",
        "  index2 = np.random.permutation([i for i in range(data_wp.shape[0])])\n",
        "  split_len1 = int(train_portion * data_np.shape[0])\n",
        "  split_len1_2 = int((1-test_portion)* data_np.shape[0])\n",
        "  split_len2 = int(train_portion * data_wp.shape[0])\n",
        "  split_len2_2 = int((1-test_portion)* data_wp.shape[0])\n",
        "\n",
        "  x_train.append(data_np[index1[:split_len1],...])\n",
        "  x_train.append(data_wp[index2[:split_len2],...])\n",
        "\n",
        "  y_train.append(label_data_np[index1[:split_len1],...])\n",
        "  y_train.append(label_data_wp[index2[:split_len2],...])\n",
        "\n",
        "  x_valid.append(data_np[index1[split_len1:split_len1_2],...])\n",
        "  x_valid.append(data_wp[index2[split_len2:split_len2_2],...])\n",
        "\n",
        "  y_valid.append(label_data_np[index1[split_len1:split_len1_2],...])\n",
        "  y_valid.append(label_data_wp[index2[split_len2:split_len2_2],...])\n",
        "\n",
        "  x_test.append(data_np[index1[split_len1_2:],...])\n",
        "  x_test.append(data_wp[index2[split_len2_2:],...])\n",
        "\n",
        "  y_test.append(label_data_np[index1[split_len1_2:],...])\n",
        "  y_test.append(label_data_wp[index2[split_len2_2:],...])\n",
        "\n",
        "  x_train = np.concatenate(x_train, axis=0)\n",
        "  y_train = np.concatenate(y_train, axis=0)\n",
        "  x_valid = np.concatenate(x_valid, axis=0)\n",
        "  y_valid = np.concatenate(y_valid, axis=0)\n",
        "  x_test = np.concatenate(x_test, axis=0)\n",
        "  y_test = np.concatenate(y_test, axis=0)\n",
        "\n",
        "  index = np.random.permutation([i for i in range(x_train.shape[0])])\n",
        "  x_train = x_train[index, ...]\n",
        "  y_train = y_train[index, ...]\n",
        "\n",
        "  # x_train = x_train.reshape(x_train.shape[0],x_train.shape[1], 1)\n",
        "  # x_valid = x_valid.reshape(x_valid.shape[0],x_valid.shape[1], 1)\n",
        "\n",
        "  return x_train, y_train, x_valid, y_valid, x_test, y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tpJ6_UnEiAu"
      },
      "outputs": [],
      "source": [
        "x_train, y_train, x_valid, y_valid, x_test, y_test = train_test_split (data_np, data_wp, label_data_np, label_data_wp,train_portion=0.7, test_portion = 0.1)\n",
        "# y_test = np.transpose(y_test)\n",
        "# y_train = np.transpose(y_train)\n",
        "# y_valid = np.transpose(y_valid)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(x_train.shape)\n",
        "print(y_valid.shape)\n",
        "print(x_valid.shape)\n",
        "print(y_test.shape)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *k-fold Split*"
      ],
      "metadata": {
        "id": "lh8i5lhYtUKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K-fold Cross Validation\n",
        "num_folds = 5\n",
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "kfold = KFold(n_splits = num_folds, shuffle = True)\n",
        "inputs  = np.concatenate((data_np, data_wp),axis = 0)\n",
        "targets = np.concatenate((label_data_np, label_data_wp), axis =0)\n",
        "#inputs = np.expand_dims(inputs, axis=-1)\n",
        "#targets = np.expand_dims(targets, axis=-1)\n",
        "print(inputs.shape)\n",
        "print(targets.shape)"
      ],
      "metadata": {
        "id": "-q1SVYhitWLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition"
      ],
      "metadata": {
        "id": "uTVQi_6fJ8iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries to def model\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, GRU, Bidirectional\n",
        "import tensorflow as tf\n",
        "\n",
        "# Apply necessay changes to all the model architetures\n",
        "\n",
        "# Example models\n",
        "# Create GRU Model\n",
        "def create_gru_model(units, input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(GRU(units=units, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(GRU(units=units, return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(100, activation='tanh'))\n",
        "    model.add(Dense(num_classes-1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create LSTM Model\n",
        "def create_lstm_model(units, input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(LSTM(units=units, return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(100, activation='tanh'))\n",
        "    model.add(Dense(num_classes-1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create BiLSTM Model\n",
        "def create_bilstm_model(units, input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units=units, return_sequences=True), input_shape=input_shape))\n",
        "    model.add(Bidirectional(LSTM(units=units, return_sequences=False)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(100, activation='tanh'))\n",
        "    model.add(Dense(num_classes-1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Choose which model to use based on user input\n",
        "def select_model(model_type, units, input_shape, num_classes):\n",
        "    if model_type == 'gru':\n",
        "        return create_gru_model(units, input_shape, num_classes)\n",
        "    elif model_type == 'lstm':\n",
        "        return create_lstm_model(units, input_shape, num_classes)\n",
        "    elif model_type == 'bilstm':\n",
        "        return create_bilstm_model(units, input_shape, num_classes)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type selected. Choose 'gru', 'lstm', or 'bilstm'.\")\n",
        "\n",
        "# User input to select model type\n",
        "model_type = input(\"Enter model type (gru, lstm, bilstm): \").strip().lower()\n",
        "\n",
        "# Define input shape and other parameters\n",
        "units = 32\n",
        "num_classes = 2\n",
        "input_shape = (x_train.shape[1], x_train.shape[2])\n",
        "\n",
        "# Create and summarize the selected model\n",
        "model = select_model(model_type, units, input_shape, num_classes)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "MShrb2D5iV5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the Number of iterations\n",
        "batch_size = 128\n",
        "epochs = 20\n",
        "\n",
        "from math import ceil\n",
        "print(\"Shape of training data is: \", x_train.shape)\n",
        "\n",
        "iterations = x_train.shape[0]/batch_size\n",
        "print(\"Number of iterations per epoch = \", ceil(iterations))"
      ],
      "metadata": {
        "id": "dYfa4AB7QWti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model fit function and saving the trained model with a file name (model_type)\n",
        "def fit_model(model):\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    file_name = f'{model_type}_model'\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            \"{name}.keras\".format(name=file_name), save_best_only=True, monitor=\"val_loss\"),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001),\n",
        "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=0),\n",
        "    ]\n",
        "\n",
        "    # Fit the model with the callbacks\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        epochs=epochs,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=(x_valid, y_valid),\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "    return history\n",
        "\n",
        "tic=timeit.default_timer()\n",
        "history = fit_model(model)\n",
        "toc=timeit.default_timer()\n",
        "print(f'Time taken for Training: {toc-tic} seconds')"
      ],
      "metadata": {
        "id": "rJlOnoeWjgJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate Model"
      ],
      "metadata": {
        "id": "HMT5c9uwim7-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEZbbZw5uX-g",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Plot train loss and validation loss\n",
        "def plot_acc_loss(history):\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(history.history['accuracy'],'r')\n",
        "    plt.plot(history.history['val_accuracy'],'b')\n",
        "    plt.title('Model Accuracy',size=14,fontweight=\"bold\")\n",
        "    plt.ylabel('Accuracy',fontweight=\"bold\")\n",
        "    plt.xlabel('epoch',fontweight=\"bold\")\n",
        "    plt.legend(['Train accuracy', 'Validation accuracy'], loc='lower right')\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(history.history['loss'],'r')\n",
        "    plt.plot(history.history['val_loss'],'b')\n",
        "    plt.title('Model Loss',size=14,fontweight=\"bold\")\n",
        "    plt.ylabel('Loss',fontweight=\"bold\")\n",
        "    plt.xlabel('epoch',fontweight=\"bold\")\n",
        "    plt.legend(['Train loss', 'Validation loss'], loc='upper right')\n",
        "    plt.grid()\n",
        "\n",
        "# Function to plot and save with model filename\n",
        "def plot_and_save(history):\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plot_acc_loss(history)  # Replace with your plotting function\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the plot with the model filename\n",
        "    plt.savefig(f'{model_type}_acc_loss.png', dpi=300)\n",
        "    print(f\"Plot saved as: {model_type}_acc_loss.png\")\n",
        "\n",
        "plot_and_save(history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(x_test,y_test)\n",
        "print(x_test.shape[0])\n",
        "print(int(x_test.shape[0]*test_acc))\n",
        "print(x_test.shape[0]-int(x_test.shape[0]*test_acc))\n",
        "scores = model.evaluate(x_test,y_test)\n",
        "print(\"Accuracy: %.4f%%\" % int(scores[1]*100))"
      ],
      "metadata": {
        "id": "8VPA0XooYPiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(x_test)\n",
        "pred_thr = np.where(prediction[:,0] >0.5, 1,0)\n",
        "pred_thr = np.expand_dims(pred_thr, axis=-1)\n",
        "# print(pred_thr)\n",
        "print(pred_thr.shape)\n",
        "count = np.array(pred_thr)\n",
        "unique, counts = np.unique(count, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "metadata": {
        "id": "sVqrzjtKYcLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Performance Evaluation Metrics*"
      ],
      "metadata": {
        "id": "CkQB6x5ZOzzF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "sBoQ52juNRua"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "bin_pred = prediction.round()\n",
        "cm = confusion_matrix(y_test, bin_pred)\n",
        "print(cm)\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "cmd = ConfusionMatrixDisplay(cm, display_labels=['RWoP','RWP'])\n",
        "cmd.plot(values_format='.4g')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIU2M_WaNRua"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import f1_score\n",
        "import math\n",
        "\n",
        "MSE= mean_squared_error(y_test, bin_pred)\n",
        "\n",
        "RMSE = math.sqrt(MSE)\n",
        "print(\"Root Mean Square Error: {:.2f}%\".format(RMSE * 100))\n",
        "\n",
        "f1score = f1_score(y_test, bin_pred)\n",
        "print('F1-Score: {:.2f}%'.format(f1score * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVICLNuANRub"
      },
      "outputs": [],
      "source": [
        "# from confusion matrix calculate Performance Evaluation Metrics\n",
        "\n",
        "# Calculate total\n",
        "total = sum(sum(cm))\n",
        "\n",
        "# Calculate metrics\n",
        "ppv = cm[1, 1] / (cm[1, 1] + cm[0, 1]) * 100\n",
        "npv = cm[0, 0] / (cm[0, 0] + cm[1, 0]) * 100\n",
        "sensitivity = cm[1, 1] / (cm[1, 1] + cm[1, 0]) * 100\n",
        "specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1]) * 100\n",
        "accuracy = (cm[1, 1] + cm[0, 0]) / total * 100\n",
        "\n",
        "# Print results rounded to two decimal places\n",
        "print('PPV (Positive Predictive Value): {:.2f}%'.format(ppv))\n",
        "print('NPV (Negative Predictive Value): {:.2f}%'.format(npv))\n",
        "print('Sensitivity (Recall): {:.2f}%'.format(sensitivity))\n",
        "print('Specificity: {:.2f}%'.format(specificity))\n",
        "print('Accuracy: {:.2f}%'.format(accuracy))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cross-Validation"
      ],
      "metadata": {
        "id": "7gH1wdZat1nY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fold_no = 1\n",
        "model.summary()\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  file_name = f'{model_type}_model_cross-valid'\n",
        "\n",
        "  callbacks = [\n",
        "      tf.keras.callbacks.ModelCheckpoint(\n",
        "          \"{name}.keras\".format(name=file_name), save_best_only=True, monitor=\"val_loss\"),\n",
        "      tf.keras.callbacks.ReduceLROnPlateau(\n",
        "          monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001),\n",
        "      tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=0),\n",
        "  ]\n",
        "\n",
        "  history = model.fit(\n",
        "          inputs[train],\n",
        "          targets[train],\n",
        "          batch_size=batch_size,\n",
        "          epochs=2,\n",
        "          validation_data=(inputs[test], targets[test]),\n",
        "          callbacks = callbacks\n",
        "        )\n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "rDznG8XLtzdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The End**\n"
      ],
      "metadata": {
        "id": "yo3OmF0WY__e"
      }
    }
  ]
}