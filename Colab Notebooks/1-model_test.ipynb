{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1bXiruOx1HQikM5n4JeH1CW3ukX_ghe2_","timestamp":1737975124970}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Import Libraries"],"metadata":{"id":"MDhxgcTvig6P"}},{"cell_type":"code","source":["import tensorflow as tf\n","print(tf.__version__)"],"metadata":{"id":"o4Q7mg714F22"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tensorflow==2.12"],"metadata":{"id":"dN7Zp8sW6qNS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","print(f\"TensorFlow version: {tf.__version__}\")"],"metadata":{"id":"R8MWe6uR7RBh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import glob\n","import time\n","import timeit\n","import pandas as pd\n","import numpy as np\n","from scipy import signal\n","# from tqdm import tqdm\n","from tqdm.notebook import tqdm\n","from pathlib import Path\n","from math import sqrt\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import KFold"],"metadata":{"id":"NC1OL14NgiQu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVqs7Gtwfe_m"},"outputs":[],"source":["# Mount Google Drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"8q3qHQ28sgMp"},"source":["# Data File Count & Duration"]},{"cell_type":"code","source":["# Define CSI_data gdrive URL\n","data_url = '/content/drive/MyDrive/CSI_Test_Data/with_person'\n","\n","raw_data = []\n","raw_data_duration = []\n","\n","# Get the list of CSV files\n","file_list = glob.glob(data_url + '/*.csv')\n","\n","print(\"Total File Count:\", len(file_list))\n","print(\"==================================================================\")\n","\n","# Process random files with\n","print(\"Processing random test data files:\")\n","for file_name in tqdm(file_list, desc=\"random test data\", unit=\"file\"):\n","  try:\n","      raw_data.append(file_name)\n","      extract_duration_data = pd.read_csv(file_name, low_memory=False)\n","      extract_duration_data = extract_duration_data['real_timestamp'].values\n","      extract_duration_data = (extract_duration_data[-1] - extract_duration_data[0]) / 60\n","      raw_data_duration.append(extract_duration_data)\n","  except Exception as e:\n","    print(f\"Error processing file {file_name}: {e}\")\n","  except ValueError as ve:\n","    print(f\"ValueError encountered: {ve}\")\n","    continue\n","\n","print(\"==================================================================\")\n","print(\"Found {} random test data files.\".format(len(raw_data)))\n","print(\"With a total duration of {} minutes.\".format(sum(raw_data_duration)))\n","print(\"==================================================================\")"],"metadata":{"id":"7kl8Ymjw_gml"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7LuJus_Ar8iY"},"source":["# Filter Design\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUyMv3g7Fcr6"},"outputs":[],"source":["def lowpass(csi_vec: np.array, cutoff: float, fs: float, order: int) -> np.array:\n","    nyq = 0.5*fs\n","    normal_cutoff = cutoff/nyq\n","    b, a = signal.butter(order, normal_cutoff, btype=\"low\", analog=False)\n","    return signal.filtfilt(b, a, csi_vec)\n","\n","def running_mean(x: np.array, N: int) -> np.array:\n","    return pd.Series(x).rolling(window=N, min_periods=1, center=True).mean().to_numpy()"]},{"cell_type":"markdown","metadata":{"id":"y720Xk0_tM0l"},"source":["#  Read File Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yueUbOIzEUD2"},"outputs":[],"source":["def read_csv(data_url, window_size, step_size):\n","\n","    scaler = StandardScaler()\n","    data_test_windowed = []\n","\n","    data_test = []\n","\n","    file_list = glob.glob(data_url + '/*.csv')\n","\n","    # File processing progress\n","    for file_name in tqdm(file_list, desc=\"Processing Files\", unit=\"file\"):\n","        print(file_name)\n","        TestData = pd.read_csv(file_name, low_memory=False)\n","        TestData = TestData['CSI_DATA'].values\n","        TestData = TestData[500:-500]\n","\n","        # Data processing progress\n","        for i in tqdm(range(TestData.shape[0]), desc=\"Processing random test data\", leave=False):\n","            try:\n","                st = TestData[i]\n","                st = st[1:-2]\n","                data_array = [int(s) for s in st.split(' ')]\n","                data_array_mag = []\n","                for k in range(0, 128, 2):\n","                    data_array_mag.append(sqrt(data_array[k]**2 + data_array[k+1]**2))\n","                data_test.append(data_array_mag)\n","            except Exception as e:\n","                #print(f\"Error encountered at row {i}: {TestData[i]}\")\n","                print(f\"Error encountered: {e}\")\n","                continue\n","            except ValueError as ve:\n","                #print(f\"ValueError encountered at row {i}: {TestData[i]}\")\n","                print(f\"ValueError encountered: {ve}\")\n","                continue\n","\n","    data_test = np.array(data_test)\n","\n","    # Window processing progress for test data\n","    for start in tqdm(range(0, data_test.shape[0] - window_size, step_size), desc=\"Creating random test-data Windows\", unit=\"window\"):\n","        end = start + window_size\n","        data_test_window = np.empty((0, 64))\n","        for j in range(start, end):\n","            data_array_mag = data_test[j]\n","            data_test_window = np.append(data_test_window, np.array([data_array_mag]), axis=0)\n","        for i in range(0, 64):\n","            data_test_window[:, i] = lowpass(data_test_window[:, i], 30, 170, 5)\n","            data_test_window[:, i] = running_mean(data_test_window[:, i], 10)\n","        data_test_window = scaler.fit_transform(data_test_window)\n","        data_test_windowed.append(data_test_window)\n","\n","    data_test = data_test_windowed\n","    return np.array(data_test)\n"]},{"cell_type":"markdown","source":["# Data Extraction"],"metadata":{"id":"pfh1VKnEUAZ8"}},{"cell_type":"code","source":["window_size = 300  # Adjust Window_Size\n","step_size = 50\n","\n","save_url_test = 'data_test_window_size={window}_step_size={step}.npy'.format(window = window_size, step = step_size)\n","save_url_test = Path(save_url_test)\n","\n","if (save_url_test.is_file() and save_url_test.is_file()):\n","  data_np = np.load(save_url_test)\n","else:\n","  file1 = 'data_test_window_size={window}_step_size={step}.npy'.format(window = window_size, step = step_size)\n","\n","  data_test = read_csv(data_url, window_size=window_size, step_size=step_size)\n","  np.save(file1,data_test)"],"metadata":{"id":"ulmhhN49I3gl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data_test.shape)"],"metadata":{"id":"S2NS0uW1KkNl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Model Load & Predict"],"metadata":{"id":"HFi-GzE4Eorm"}},{"cell_type":"code","source":["import keras\n","\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","import tensorflow as tf\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","\n","from keras.models import load_model\n","model = load_model('/content/drive/MyDrive/Trained_Model/lstm_model_300.h5')\n","model.summary()"],"metadata":{"id":"ZdmYlf-u1qVw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","start_time = time.time()\n","predict = model.predict(data_test)\n","end_time = time.time()\n","print(\"--- %s seconds ---\" % (time.time() - start_time))\n","# print(predict)\n","# print(predict.shape)"],"metadata":{"id":"Lyids2FREgaO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CPU processing Time\n","start_time = time.process_time()\n","predict = model.predict(data_test)\n","end_time = time.process_time()\n","print(\"Testing time per block: %.2f ms\" %((end_time-start_time)*1000/data_test.shape[0]))"],"metadata":{"id":"jCRb_N7iH6tt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_thr = np.where(predict[:,0] >0.5, 1,0)\n","pred_thr = np.expand_dims(pred_thr, axis=-1)\n","# print(pred_thr)\n","print(pred_thr.shape)\n","count = np.array(pred_thr)\n","unique, counts = np.unique(count, return_counts=True)\n","dict(zip(unique, counts))\n","print(\"With Person #blocks (1):\", counts[1])\n","print(\"No Person #blocks (0):\", counts[0])"],"metadata":{"id":"IREbZNFJIAqw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **The End**\n"],"metadata":{"id":"yo3OmF0WY__e"}}]}